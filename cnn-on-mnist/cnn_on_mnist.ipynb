{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">Deep Learning   </h1>\n",
    "<h1 style=\"text-align:center\"> Lab Session 2 - 1.5 Hours </h1>\n",
    "<h1 style=\"text-align:center\"> Convolutional Neural Network (CNN) for Handwritten Digits Recognition</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Group name: </b> deeplearn8 <br>\n",
    "<b> Student1: </b> Claudio SCALZO <br>\n",
    "<b> Student2: </b> Luca LOMBARDO <br>\n",
    " \n",
    " \n",
    "The aim of this session is to practice with Convolutional Neural Networks. Each group should fill and run appropriate notebook cells. \n",
    "\n",
    "\n",
    "Generate your final report (export as HTML) and upload it on the submission website http://bigfoot-m1.eurecom.fr/teachingsub/login (using your deeplearnXX/password). Do not forget to run all your cells before generating your final report and do not forget to include the names of all participants in the group. The lab session should be completed and submitted by May 30th 2018 (23:59:59 CET)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous Lab Session, you built a Multilayer Perceptron for recognizing hand-written digits from the MNIST data-set. The best achieved accuracy on testing data was about 97%. Can you do better than these results using a deep CNN ?\n",
    "In this Lab Session, you will build, train and optimize in TensorFlow one of the early Convolutional Neural Networks,  **LeNet-5**, to go to more than 99% of accuracy. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST Data in TensorFlow\n",
    "Run the cell below to load the MNIST data that comes with TensorFlow. You will use this data in **Section 1** and **Section 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Images Shape:   (28, 28, 1)\n",
      "Training Set:   55000 samples\n",
      "Validation Set: 5000 samples\n",
      "Test Set:       10000 samples\n"
     ]
    }
   ],
   "source": [
    "# SUPPRESS WARNINGS\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# IMPORT LIBRARIES\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from time import time\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "# LOAD DATA (RESHAPING THE IMAGES)\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "X_train, y_train = mnist.train.images.reshape((-1,28,28,1)), mnist.train.labels\n",
    "X_val, y_val     = mnist.validation.images.reshape((-1,28,28,1)), mnist.validation.labels\n",
    "X_test, y_test   = mnist.test.images.reshape((-1,28,28,1)), mnist.test.labels\n",
    "\n",
    "\n",
    "# PRINT SHAPES\n",
    "print(\"Images Shape:   {}\".format(X_train[0].shape))\n",
    "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
    "print(\"Validation Set: {} samples\".format(len(X_val)))\n",
    "print(\"Test Set:       {} samples\".format(len(X_test)))\n",
    "\n",
    "\n",
    "# DECLARE NOISE\n",
    "epsilon = 1e-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 : Neural Network in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting with CNN, let's train and test in TensorFlow the example\n",
    "**y=softmax(Wx+b)** seen in the first lab. \n",
    "\n",
    "This model reaches an accuracy of about 92 %.\n",
    "You will also learn how to launch the TensorBoard https://www.tensorflow.org/get_started/summaries_and_tensorboard to visualize the computation graph, statistics and learning curves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 1 </b> : Read carefully the code in the cell below. Run it to perform training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01  =====>  Loss= 1.289137611\n",
      "Epoch: 02  =====>  Loss= 0.732774512\n",
      "Epoch: 03  =====>  Loss= 0.600600310\n",
      "Epoch: 04  =====>  Loss= 0.536735662\n",
      "Epoch: 05  =====>  Loss= 0.497956680\n",
      "Epoch: 06  =====>  Loss= 0.471396519\n",
      "Epoch: 07  =====>  Loss= 0.451227631\n",
      "Epoch: 08  =====>  Loss= 0.435891133\n",
      "Epoch: 09  =====>  Loss= 0.423669444\n",
      "Epoch: 10  =====>  Loss= 0.413211194\n",
      "Epoch: 11  =====>  Loss= 0.404243609\n",
      "Epoch: 12  =====>  Loss= 0.396904861\n",
      "Epoch: 13  =====>  Loss= 0.390230405\n",
      "Epoch: 14  =====>  Loss= 0.384440295\n",
      "Epoch: 15  =====>  Loss= 0.379369415\n",
      "Epoch: 16  =====>  Loss= 0.374595078\n",
      "Epoch: 17  =====>  Loss= 0.370155263\n",
      "Epoch: 18  =====>  Loss= 0.366575314\n",
      "Epoch: 19  =====>  Loss= 0.362992976\n",
      "Epoch: 20  =====>  Loss= 0.359501431\n",
      "Epoch: 21  =====>  Loss= 0.356614839\n",
      "Epoch: 22  =====>  Loss= 0.353882593\n",
      "Epoch: 23  =====>  Loss= 0.350913259\n",
      "Epoch: 24  =====>  Loss= 0.348808232\n",
      "Epoch: 25  =====>  Loss= 0.346653387\n",
      "Epoch: 26  =====>  Loss= 0.344549811\n",
      "Epoch: 27  =====>  Loss= 0.342343539\n",
      "Epoch: 28  =====>  Loss= 0.340498662\n",
      "Epoch: 29  =====>  Loss= 0.338493211\n",
      "Epoch: 30  =====>  Loss= 0.336767976\n",
      "Epoch: 31  =====>  Loss= 0.335000932\n",
      "Epoch: 32  =====>  Loss= 0.333733021\n",
      "Epoch: 33  =====>  Loss= 0.332103749\n",
      "Epoch: 34  =====>  Loss= 0.330575183\n",
      "Epoch: 35  =====>  Loss= 0.329263096\n",
      "Epoch: 36  =====>  Loss= 0.327947930\n",
      "Epoch: 37  =====>  Loss= 0.326483817\n",
      "Epoch: 38  =====>  Loss= 0.325537870\n",
      "Epoch: 39  =====>  Loss= 0.323984801\n",
      "Epoch: 40  =====>  Loss= 0.323061464\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9162\n"
     ]
    }
   ],
   "source": [
    "# PARAMETERS\n",
    "learning_rate = 0.01\n",
    "training_epochs = 40\n",
    "batch_size = 128\n",
    "display_step = 1\n",
    "logs_path = 'log_files/'  # useful for tensorboard\n",
    "\n",
    "\n",
    "# PLACEHOLDERS\n",
    "# Images with 784 pixels\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='InputData')\n",
    "# 0-9 digits recognition, 10 classes\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "\n",
    "\n",
    "# WEIGHTS AND BIAS\n",
    "W = tf.Variable(tf.zeros([784, 10]), name='Weights')\n",
    "b = tf.Variable(tf.zeros([10]), name='Bias')\n",
    "\n",
    "\n",
    "# MODEL ARCHITECTURE\n",
    "# Model (softmax activation)\n",
    "with tf.name_scope('Model'):\n",
    "    pred = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "# Loss function (cross-entropy)\n",
    "with tf.name_scope('Loss'):\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.clip_by_value(pred, epsilon, 1.0)), reduction_indices=1))\n",
    "    \n",
    "# Optimizer (SGD)\n",
    "with tf.name_scope('SGD'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "    \n",
    "# Accuracy\n",
    "with tf.name_scope('Accuracy'):\n",
    "    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "\n",
    "\n",
    "# INIT SUMMARY\n",
    "init = tf.global_variables_initializer()\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "tf.summary.scalar(\"Accuracy\", acc)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "# LAUNCH THE SESSION\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size, shuffle=(i==0))\n",
    "\n",
    "            _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                     feed_dict={x: batch_xs, y: batch_ys})\n",
    "            \n",
    "            summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "            \n",
    "            avg_cost += c / total_batch\n",
    "        \n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%02d' % (epoch+1), \" =====>  Loss=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    summary_writer.flush()\n",
    "\n",
    "    print(\"Accuracy:\", acc.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 2  </b>: Using Tensorboard, we can  now visualize the created graph, giving you an overview of your architecture and how all of the major components  are connected. You can also see and analyse the learning curves. \n",
    "\n",
    "To launch tensorBoard: \n",
    "- Open a Terminal and run the command line **\"tensorboard --logdir=lab_2/log_files/\"**\n",
    "- Click on \"Tensorboard web interface\" in Zoe  \n",
    "\n",
    "\n",
    "Enjoy It !! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 : The 99% MNIST Challenge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 1 </b> : LeNet5 implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now more familar with **TensorFlow** and **TensorBoard**. In this section, you are to build, train and test the baseline [LeNet-5](http://yann.lecun.com/exdb/lenet/)  model for the MNIST digits recognition problem.  \n",
    "\n",
    "Then, you will make some optimizations to get more than 99% of accuracy.\n",
    "\n",
    "For more informations, have a look at this list of results: http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/claudioscalzo/deep-learning/raw/a0913dc47c87534c5659269579a5e35aaa16f0a7/cnn-on-mnist/MNIST_figures/lenet.png\" width=\"800\" height=\"600\" align=\"center\">\n",
    "<center><span>Figure 1: Lenet-5</span></center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The LeNet architecture takes a 28x28xC image as input, where C is the number of color channels. Since MNIST images are grayscale, C is 1 in this case.\n",
    "\n",
    "--------------------------\n",
    "**Layer 1 - Convolution (5x5):** The output shape should be 28x28x6. **Activation:** ReLU. **MaxPooling:** The output shape should be 14x14x6.\n",
    "\n",
    "**Layer 2 - Convolution (5x5):** The output shape should be 10x10x16. **Activation:** ReLU. **MaxPooling:** The output shape should be 5x5x16.\n",
    "\n",
    "**Flatten:** Flatten the output shape of the final pooling layer such that it's 1D instead of 3D.  You may need to use tf.reshape.\n",
    "\n",
    "**Layer 3 - Fully Connected:** This should have 120 outputs. **Activation:** ReLU.\n",
    "\n",
    "**Layer 4 - Fully Connected:** This should have 84 outputs. **Activation:** ReLU.\n",
    "\n",
    "**Layer 5 - Fully Connected:** This should have 10 outputs. **Activation:** softmax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.1 </b>  Implement the Neural Network architecture described above.\n",
    "For that, your will use classes and functions from  https://www.tensorflow.org/api_docs/python/tf/nn. \n",
    "\n",
    "We give you some helper functions for weigths and bias initilization. Also you can refer to section 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FUNCTIONS FOR WEIGHT AND BIAS INITIALIZATIONS --- #\n",
    "# WEIGHTS\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "# BIASES\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0., shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeNet5_Model(images):    \n",
    "    \n",
    "    # --- INTERNAL MODEL DEFINITION --- #\n",
    "    with tf.name_scope(\"model\"):\n",
    "        \n",
    "        # CONVOLUTIONAL 1 LAYER\n",
    "        with tf.name_scope(\"conv1\"):\n",
    "            \n",
    "            # Convolution (28x28x1 -> CONV1 -> 28x28x6)\n",
    "            conv1 = tf.nn.conv2d(input=images,\n",
    "                                 filter=weight_variable([5,5,1,6]),\n",
    "                                 strides=[1,1,1,1],\n",
    "                                 padding='SAME')\n",
    "            \n",
    "            # Activation\n",
    "            act1 = tf.nn.relu(conv1 + bias_variable([6]))\n",
    "            \n",
    "            # Pooling (28x28x6 -> MAXPOOL1 -> 14x14x6)\n",
    "            pool1 = tf.nn.max_pool(act1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    \n",
    "    \n",
    "        # CONVOLUTIONAL 2 LAYER\n",
    "        with tf.name_scope('conv2'):\n",
    "            \n",
    "            # Convolution (14x14x6 -> CONV2 -> 10x10x16)\n",
    "            conv2 = tf.nn.conv2d(input=pool1,\n",
    "                                 filter=weight_variable([5,5,6,16]),\n",
    "                                 strides=[1,1,1,1],\n",
    "                                 padding='VALID')\n",
    "            \n",
    "            # Activation\n",
    "            act2 = tf.nn.relu(conv2 + bias_variable([16]))\n",
    "            \n",
    "            # Pooling (10x10x16 -> MAXPOOL2 -> 5x5x16)\n",
    "            pool2 = tf.nn.max_pool(act2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')   \n",
    "        \n",
    "        \n",
    "        # FLATTEN LAYER\n",
    "        with tf.name_scope('flatten'):\n",
    "            \n",
    "            # Flatten (5x5x16 -> 400)\n",
    "            flat = tf.contrib.layers.flatten(pool2)\n",
    "        \n",
    "        \n",
    "        # FULLY CONNECTED LAYER 3\n",
    "        with tf.name_scope('layer3'):\n",
    "            \n",
    "            # Activation (400 -> FC3 -> 120)\n",
    "            hidden3 = tf.nn.relu(tf.matmul(flat, weight_variable([400,120])) + bias_variable([120]))\n",
    "            \n",
    "            \n",
    "        # FULLY CONNECTED LAYER 4\n",
    "        with tf.name_scope('layer4'):\n",
    "            \n",
    "            # Activation (120 -> FC4 -> 84)\n",
    "            hidden4 = tf.nn.relu(tf.matmul(hidden3, weight_variable([120,84])) + bias_variable([84]))\n",
    "            \n",
    "            \n",
    "        # FULLY CONNECTED LAYER 5\n",
    "        with tf.name_scope('layer5'):\n",
    "            \n",
    "            # Activation (84 -> FC5 -> 10)\n",
    "            predictions = tf.nn.softmax(tf.matmul(hidden4, weight_variable([84,10])) + bias_variable([10]))\n",
    "        \n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-success'>\n",
    "The LeNet architecture now has been declared. The filter sizes and strides have been chosen in order to respect requested sizes between the various layers, remembering the formula:\n",
    "$$ I_{i+1} = \\frac{I_i - F + 2P}{S} + 1 $$\n",
    "\n",
    "<ul>\n",
    "    <li><b>Convolutional Layer 1</b>: 28x28x1 ----- <i>[5x5]</i> -----> 28x28x6</li>\n",
    "    we've chosen $S = 1$. The padding will be $P = 2$ (chosen by the TensorFlow option <tt>'SAME'</tt>).<br><br>\n",
    "    <li><b>MaxPooling Layer 1</b>: 28x28x6 -----> 14x14x6</li>\n",
    "    To avoid \"intersections\" between the various pools, the kernel size value has been chosen also for the stride, so:\n",
    "    $ 14 = \\frac{28 - K}{K} + 1 $. We've chosen $ K = 2 $ as feasible value.<br><br>\n",
    "    <li><b>Convolutional Layer 2</b>: 14x14x6 ----- <i>[5x5]</i> ----->10x10x16</li>\n",
    "    $I_{i+1} = \\frac{I_i - F}{S} + 1 = \\frac{14 - 5}{S} + 1 $, so $ S = 1 $ is a feasible value.<br><br>\n",
    "    <li><b>MaxPooling Layer 2</b>: 10x10x16 -----> 5x5x16</li>\n",
    "    Also in this case, to avoid \"intersections\" between the various pools, the kernel size value has been chosen also for the stride, so:\n",
    "    $ 5 = \\frac{10 - K}{K} + 1 $. We've chosen $ K = 2 $ as feasible value.\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.2. </b>  Calculate the number of parameters of this model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 61706\n"
     ]
    }
   ],
   "source": [
    "# COMPUTE THE NUMBER OF PARAMETERS\n",
    "count = np.sum([np.prod(v.shape.as_list()) for v in tf.trainable_variables()])\n",
    "\n",
    "# PRINT THE RESULT\n",
    "print(\"Total number of parameters:\", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-success'>\n",
    "The number of trainable parameters is <b>61706</b>. This number has been computed thanks to the <tt>tf.trainable_variables()</tt> function available in TensorFlow:<br>\n",
    "<br>\n",
    "- For each variable, the shape is computed.<br>\n",
    "- The components of the shape are multiplied, so the number of parameters regarding that shape is obtained.<br>\n",
    "- All the parameters regarding each shape are then summed together.<br>\n",
    "<br>\n",
    "The total number so, can be calculated also by hand:\n",
    "<code>\n",
    "5 x 5 x 1 x 6    +\n",
    "6                +\n",
    "5 x 5 x 6 x 16   +\n",
    "16               +\n",
    "400 x 120        +\n",
    "120              +\n",
    "120 x 84         +\n",
    "84               +\n",
    "84 x 10          +\n",
    "10               =\n",
    "<b>61706</b>\n",
    "</code>\n",
    "\n",
    "\n",
    "<br>\n",
    "&#42;<i>note: this cell, even if it's located before, has been executed <b>after</b> the following cell, to be run with the right instance of the model (the CNN), and not with the one of the classic neural network.<i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.3. </b>  Define your model, its accuracy and the loss function according to the following parameters (you can look at Section 1 to see what is expected):\n",
    "\n",
    "     Learning rate: 0.001\n",
    "     Loss Function: Cross-entropy\n",
    "     Optimizer: tf.train.GradientDescentOptimizer\n",
    "     Number of epochs: 40\n",
    "     Batch size: 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESET THE GRAPH AND CLEAR LOGS\n",
    "! rm -rf ./log_files\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# PARAMETERS\n",
    "learning_rate = 0.001\n",
    "training_epochs = 40\n",
    "batch_size = 128\n",
    "loss_display_step = 1\n",
    "acc_display_step = 10\n",
    "logs_path = 'log_files/'\n",
    "\n",
    "\n",
    "# PLACEHOLDERS\n",
    "x = tf.placeholder(tf.float32, [None,28,28,1], name='input')\n",
    "y = tf.placeholder(tf.float32, [None,10], name='output')\n",
    "\n",
    "\n",
    "# MODEL DEFINITION\n",
    "# Model structure\n",
    "with tf.name_scope('model'):\n",
    "    pred = LeNet5_Model(images=x)\n",
    "    \n",
    "# Loss\n",
    "with tf.name_scope('loss'):\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.clip_by_value(pred, epsilon, 1.0)), reduction_indices=1))\n",
    "\n",
    "# Optimizer\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.4. </b>  Implement the evaluation function for accuracy computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION FUNCTION\n",
    "def evaluate(logits, labels):\n",
    "    \n",
    "    correct = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "    return tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "# Accuracy\n",
    "with tf.name_scope('accuracy'):\n",
    "    acc = evaluate(pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.5. </b>  Implement training pipeline and run the training data through it to train the model.\n",
    "\n",
    "- Before each epoch, shuffle the training set. \n",
    "- Print the loss per mini batch and the training/validation accuracy per epoch. (Display results every 100 epochs)\n",
    "- Save the model after training\n",
    "- Print after training the final testing accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN FUNCTION\n",
    "def train(init, sess, logs_path, n_epochs, batch_size, optimizer, cost, merged_summary_op):\n",
    "    \n",
    "    # Start\n",
    "    print(\"[OPTIMIZATION STARTED]\\n\")\n",
    "    startTime = time()\n",
    "    sess.run(init)\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    \n",
    "    # Training\n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        # Init\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        \n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            # Next batch\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size, shuffle=(i==0))\n",
    "            batch_xs = batch_xs.reshape(batch_size,28,28,1)\n",
    "            \n",
    "            # Run optimization\n",
    "            _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                     feed_dict={x: batch_xs, y: batch_ys})\n",
    "            \n",
    "            # Write logs\n",
    "            summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "            \n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        \n",
    "        # Display logs\n",
    "        if (epoch+1) % loss_display_step == 0:\n",
    "            print(\"Epoch:\", '%02d' % (epoch+1), \" =====>  Loss =\", \"{:.4f}\".format(avg_cost))\n",
    "        \n",
    "        if (epoch+1) % acc_display_step == 0:\n",
    "            acc_train = acc.eval({x: X_train, y: y_train})\n",
    "            acc_val = acc.eval({x: X_val, y: y_val})\n",
    "            \n",
    "            print(\"Train accuracy =\", \"{:.2f}%\".format(acc_train*100),\n",
    "                  \" | \",\n",
    "                  \"Validation accuracy =\", \"{:.2f}%\\n\".format(acc_val*100))\n",
    "            \n",
    "    \n",
    "    endTime = time()\n",
    "    print(\"[OPTIMIZATION FINISHED]\")\n",
    "    print(\"Training time =\", \"{:.3f} seconds\".format(endTime-startTime))\n",
    "    summary_writer.flush()\n",
    "    \n",
    "    \n",
    "    # --- ACCURACY ------------------------------------------------------ #\n",
    "    acc_test = acc.eval({x: X_test, y: y_test})\n",
    "    print(\"Test accuracy =\", \"{:.2f}%\".format(acc_test*100))\n",
    "    \n",
    "    return endTime-startTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='label label-success'>LeNet architecture (SGD optimizer)</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OPTIMIZATION STARTED]\n",
      "\n",
      "Epoch: 01  =====>  Loss = 2.2840\n",
      "Epoch: 02  =====>  Loss = 2.2527\n",
      "Epoch: 03  =====>  Loss = 2.2014\n",
      "Epoch: 04  =====>  Loss = 2.0847\n",
      "Epoch: 05  =====>  Loss = 1.7741\n",
      "Epoch: 06  =====>  Loss = 1.2123\n",
      "Epoch: 07  =====>  Loss = 0.8191\n",
      "Epoch: 08  =====>  Loss = 0.6432\n",
      "Epoch: 09  =====>  Loss = 0.5463\n",
      "Epoch: 10  =====>  Loss = 0.4819\n",
      "Train accuracy = 86.83%  |  Validation accuracy = 87.40%\n",
      "\n",
      "Epoch: 11  =====>  Loss = 0.4350\n",
      "Epoch: 12  =====>  Loss = 0.3990\n",
      "Epoch: 13  =====>  Loss = 0.3706\n",
      "Epoch: 14  =====>  Loss = 0.3478\n",
      "Epoch: 15  =====>  Loss = 0.3285\n",
      "Epoch: 16  =====>  Loss = 0.3123\n",
      "Epoch: 17  =====>  Loss = 0.2980\n",
      "Epoch: 18  =====>  Loss = 0.2856\n",
      "Epoch: 19  =====>  Loss = 0.2742\n",
      "Epoch: 20  =====>  Loss = 0.2643\n",
      "Train accuracy = 92.30%  |  Validation accuracy = 92.60%\n",
      "\n",
      "Epoch: 21  =====>  Loss = 0.2551\n",
      "Epoch: 22  =====>  Loss = 0.2464\n",
      "Epoch: 23  =====>  Loss = 0.2386\n",
      "Epoch: 24  =====>  Loss = 0.2314\n",
      "Epoch: 25  =====>  Loss = 0.2246\n",
      "Epoch: 26  =====>  Loss = 0.2185\n",
      "Epoch: 27  =====>  Loss = 0.2126\n",
      "Epoch: 28  =====>  Loss = 0.2072\n",
      "Epoch: 29  =====>  Loss = 0.2020\n",
      "Epoch: 30  =====>  Loss = 0.1972\n",
      "Train accuracy = 94.26%  |  Validation accuracy = 94.52%\n",
      "\n",
      "Epoch: 31  =====>  Loss = 0.1926\n",
      "Epoch: 32  =====>  Loss = 0.1881\n",
      "Epoch: 33  =====>  Loss = 0.1839\n",
      "Epoch: 34  =====>  Loss = 0.1800\n",
      "Epoch: 35  =====>  Loss = 0.1762\n",
      "Epoch: 36  =====>  Loss = 0.1724\n",
      "Epoch: 37  =====>  Loss = 0.1693\n",
      "Epoch: 38  =====>  Loss = 0.1658\n",
      "Epoch: 39  =====>  Loss = 0.1627\n",
      "Epoch: 40  =====>  Loss = 0.1597\n",
      "Train accuracy = 95.27%  |  Validation accuracy = 95.62%\n",
      "\n",
      "[OPTIMIZATION FINISHED]\n",
      "Training time = 1065.189 seconds\n",
      "Test accuracy = 95.66%\n"
     ]
    }
   ],
   "source": [
    "# MAKE SUMMARIES\n",
    "init = tf.global_variables_initializer()\n",
    "tf.summary.scalar(\"loss-LeNet\", cost)\n",
    "tf.summary.scalar(\"accuracy-LeNet\", acc)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "# LAUNCH SESSION\n",
    "with tf.Session() as sess:\n",
    "    train(init, sess, logs_path, training_epochs, batch_size, optimizer, cost, merged_summary_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.6 </b> : Use TensorBoard to visualise and save loss and accuracy curves. \n",
    "You will save figures in the folder **\"lab_2/MNIST_figures\"** and display them in your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-success'>\n",
    "This is the graph showing the general architecture of the network:<br>\n",
    "<img src=\"https://github.com/claudioscalzo/deep-learning/raw/a0913dc47c87534c5659269579a5e35aaa16f0a7/cnn-on-mnist/MNIST_figures/graph1.png\" width=\"600px\" align=\"center\"><br>\n",
    "<br>\n",
    "This is a zoom on the first convolutional layer:<br>\n",
    "<img src=\"https://github.com/claudioscalzo/deep-learning/raw/a0913dc47c87534c5659269579a5e35aaa16f0a7/cnn-on-mnist/MNIST_figures/graph2.png\" width=\"600px\" align=\"center\"><br>\n",
    "<br>\n",
    "<img src=\"https://github.com/claudioscalzo/deep-learning/raw/a0913dc47c87534c5659269579a5e35aaa16f0a7/cnn-on-mnist/MNIST_figures/loss.png\" width=\"1000px\"><br>\n",
    "<img src=\"https://github.com/claudioscalzo/deep-learning/raw/a0913dc47c87534c5659269579a5e35aaa16f0a7/cnn-on-mnist/MNIST_figures/accuracy.png\" width=\"1000px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 2 </b> : LeNET 5 Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.2.1 </b>\n",
    "\n",
    "- Retrain your network with AdamOptimizer and then fill the table above:\n",
    "\n",
    "\n",
    "| Optimizer            |  Gradient Descent  |    AdamOptimizer    |\n",
    "|----------------------|--------------------|---------------------|\n",
    "| Testing Accuracy     |       95.66%       |       98.98%        |       \n",
    "| Training Time        |      909 sec       |      877 sec        |  \n",
    "\n",
    "- Which optimizer gives the best accuracy on test data?\n",
    "\n",
    "<div class='alert alert-success'>\n",
    "The adam optimizer gives a <b>better</b> accuracy. It takes a little bit of time more than the model with the standard SGD, but is able to classify in a better way the <tt>MNIST</tt> dataset.<br>\n",
    "<img src=\"https://github.com/claudioscalzo/deep-learning/raw/f429d807dbd2b5171f8e9e0acc62fe0b8bdc1fbb/cnn-on-mnist/MNIST_figures/adam.png\" width=300px>\n",
    "<br>\n",
    "The <b>Adam</b> optimizer, indeed, is a combination of the improvements brought by other optimizers: it adds momentum to the RMSProp optimizer. It adapts not only the learning rates basing it on the first moments (the mean) as in RMSProp, but it also makes use of the average of the second moments of the gradients (the variance).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='label label-success'>LeNet architecture (Adam optimizer)</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OPTIMIZATION STARTED]\n",
      "\n",
      "Epoch: 01  =====>  Loss = 0.3601\n",
      "Epoch: 02  =====>  Loss = 0.0956\n",
      "Epoch: 03  =====>  Loss = 0.0645\n",
      "Epoch: 04  =====>  Loss = 0.0490\n",
      "Epoch: 05  =====>  Loss = 0.0401\n",
      "Epoch: 06  =====>  Loss = 0.0331\n",
      "Epoch: 07  =====>  Loss = 0.0277\n",
      "Epoch: 08  =====>  Loss = 0.0233\n",
      "Epoch: 09  =====>  Loss = 0.0204\n",
      "Epoch: 10  =====>  Loss = 0.0194\n",
      "Train accuracy = 99.44%  |  Validation accuracy = 98.78%\n",
      "\n",
      "Epoch: 11  =====>  Loss = 0.0165\n",
      "Epoch: 12  =====>  Loss = 0.0133\n",
      "Epoch: 13  =====>  Loss = 0.0131\n",
      "Epoch: 14  =====>  Loss = 0.0120\n",
      "Epoch: 15  =====>  Loss = 0.0096\n",
      "Epoch: 16  =====>  Loss = 0.0091\n",
      "Epoch: 17  =====>  Loss = 0.0095\n",
      "Epoch: 18  =====>  Loss = 0.0076\n",
      "Epoch: 19  =====>  Loss = 0.0074\n",
      "Epoch: 20  =====>  Loss = 0.0068\n",
      "Train accuracy = 99.87%  |  Validation accuracy = 99.04%\n",
      "\n",
      "Epoch: 21  =====>  Loss = 0.0055\n",
      "Epoch: 22  =====>  Loss = 0.0092\n",
      "Epoch: 23  =====>  Loss = 0.0038\n",
      "Epoch: 24  =====>  Loss = 0.0063\n",
      "Epoch: 25  =====>  Loss = 0.0069\n",
      "Epoch: 26  =====>  Loss = 0.0044\n",
      "Epoch: 27  =====>  Loss = 0.0048\n",
      "Epoch: 28  =====>  Loss = 0.0066\n",
      "Epoch: 29  =====>  Loss = 0.0039\n",
      "Epoch: 30  =====>  Loss = 0.0058\n",
      "Train accuracy = 99.82%  |  Validation accuracy = 99.08%\n",
      "\n",
      "Epoch: 31  =====>  Loss = 0.0029\n",
      "Epoch: 32  =====>  Loss = 0.0028\n",
      "Epoch: 33  =====>  Loss = 0.0068\n",
      "Epoch: 34  =====>  Loss = 0.0035\n",
      "Epoch: 35  =====>  Loss = 0.0036\n",
      "Epoch: 36  =====>  Loss = 0.0055\n",
      "Epoch: 37  =====>  Loss = 0.0016\n",
      "Epoch: 38  =====>  Loss = 0.0024\n",
      "Epoch: 39  =====>  Loss = 0.0053\n",
      "Epoch: 40  =====>  Loss = 0.0045\n",
      "Train accuracy = 99.80%  |  Validation accuracy = 98.80%\n",
      "\n",
      "[OPTIMIZATION FINISHED]\n",
      "Training time = 877.757 seconds\n",
      "Test accuracy = 98.98%\n"
     ]
    }
   ],
   "source": [
    "# RESET THE GRAPH AND CLEAR LOGS\n",
    "! rm -rf ./log_files\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# PARAMETERS\n",
    "learning_rate = 0.001\n",
    "training_epochs = 40\n",
    "batch_size = 128\n",
    "loss_display_step = 1\n",
    "acc_display_step = 10\n",
    "logs_path = 'log_files/'\n",
    "\n",
    "\n",
    "# PLACEHOLDERS\n",
    "x = tf.placeholder(tf.float32, [None,28,28,1], name='input')\n",
    "y = tf.placeholder(tf.float32, [None,10], name='output')\n",
    "\n",
    "\n",
    "# MODEL DEFINITION\n",
    "# Model structure\n",
    "with tf.name_scope('model'):\n",
    "    pred = LeNet5_Model(images=x)\n",
    "    \n",
    "# Loss\n",
    "with tf.name_scope('loss'):\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.clip_by_value(pred, epsilon, 1.0)), reduction_indices=1))\n",
    "\n",
    "# Optimizer\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "with tf.name_scope('accuracy'):\n",
    "    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "    \n",
    "\n",
    "# MAKE SUMMARIES\n",
    "init = tf.global_variables_initializer()\n",
    "tf.summary.scalar(\"loss-LeNet\", cost)\n",
    "tf.summary.scalar(\"accuracy-LeNet\", acc)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "# LAUNCH SESSION\n",
    "with tf.Session() as sess:\n",
    "    train(init, sess, logs_path, training_epochs, batch_size, optimizer, cost, merged_summary_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.2.2</b> Try to add dropout (keep_prob = 0.75) before the first fully connected layer. You will use tf.nn.dropout for that purpose. What accuracy do you achieve on testing data?\n",
    "\n",
    "**Accuracy achieved on testing data:** <b><u>98.99%</u></b> (with 0.75-dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-success'>\n",
    "The accuracy obtained with the dropout layer is almost the same, just slightly better than the LeNet CNN without it. Dropout is, in general, useful to generalize the network avoid overfitting: the performances will be a bit worse on the training data but, in general, on validation and test data will be less overfitted, so the performances could be better. In this case, the obtained accuracy on test set is almost the same, just slightly better (98.98% without dropout vs 98.99% with dropout), while on the train data the accuracy of the model with dropout is of course a bit worse.<br>\n",
    "The computational time is slightly smaller: maybe this is due to a TensorFlow code optimization, which can avoid to compute the hidden outputs of dropped neurons.<br>\n",
    "<br>\n",
    "The implementation is the following:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeNet5_Model_Dropout(images):\n",
    "    \n",
    "    # --- INTERNAL MODEL DEFINITION --- #\n",
    "    with tf.name_scope(\"model\"):\n",
    "        \n",
    "        # CONVOLUTIONAL 1 LAYER\n",
    "        with tf.name_scope(\"conv1\"):\n",
    "\n",
    "            # Convolution (28x28x1 -> CONV1 -> 28x28x6)\n",
    "            conv1 = tf.nn.conv2d(input=images,\n",
    "                                 filter=weight_variable([5,5,1,6]),\n",
    "                                 strides=[1,1,1,1],\n",
    "                                 padding='SAME')\n",
    "\n",
    "            # Activation\n",
    "            act1 = tf.nn.relu(conv1 + bias_variable([6]))\n",
    "\n",
    "            # Pooling (28x28x6 -> MAXPOOL1 -> 14x14x6)\n",
    "            pool1 = tf.nn.max_pool(act1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "\n",
    "\n",
    "        # CONVOLUTIONAL 2 LAYER\n",
    "        with tf.name_scope('conv2'):\n",
    "\n",
    "            # Convolution (14x14x6 -> CONV2 -> 10x10x16)\n",
    "            conv2 = tf.nn.conv2d(input=pool1,\n",
    "                                 filter=weight_variable([5,5,6,16]),\n",
    "                                 strides=[1,1,1,1],\n",
    "                                 padding='VALID')\n",
    "\n",
    "            # Activation\n",
    "            act2 = tf.nn.relu(conv2 + bias_variable([16]))\n",
    "\n",
    "            # Pooling (10x10x16 -> MAXPOOL2 -> 5x5x16)\n",
    "            pool2 = tf.nn.max_pool(act2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')   \n",
    "\n",
    "\n",
    "        # FLATTEN LAYER\n",
    "        with tf.name_scope('flatten'):\n",
    "\n",
    "            # Flatten (5x5x16 -> 400)\n",
    "            flat = tf.contrib.layers.flatten(pool2)\n",
    "\n",
    "\n",
    "        # DROPOUT LAYER\n",
    "        with tf.name_scope('dropout'):\n",
    "\n",
    "            # Dropout (0.75)\n",
    "            flat_drop = tf.nn.dropout(flat, keep_prob = 0.75)\n",
    "\n",
    "\n",
    "        # FULLY CONNECTED LAYER 3\n",
    "        with tf.name_scope('layer3'):\n",
    "\n",
    "            # Activation (400 -> FC3 -> 120)\n",
    "            hidden3 = tf.nn.relu(tf.matmul(flat_drop, weight_variable([400,120])) + bias_variable([120]))\n",
    "\n",
    "\n",
    "        # FULLY CONNECTED LAYER 4\n",
    "        with tf.name_scope('layer4'):\n",
    "\n",
    "            # Activation (120 -> FC4 -> 84)\n",
    "            hidden4 = tf.nn.relu(tf.matmul(hidden3, weight_variable([120,84])) + bias_variable([84]))\n",
    "\n",
    "\n",
    "        # FULLY CONNECTED LAYER 5\n",
    "        with tf.name_scope('layer5'):\n",
    "\n",
    "            # Activation (84 -> FC5 -> 10)\n",
    "            predictions = tf.nn.softmax(tf.matmul(hidden4, weight_variable([84,10])) + bias_variable([10]))\n",
    "\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='label label-success'>LeNet architecture with Dropout (Adam optimizer)</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OPTIMIZATION STARTED]\n",
      "\n",
      "Epoch: 01  =====>  Loss = 0.3772\n",
      "Epoch: 02  =====>  Loss = 0.1118\n",
      "Epoch: 03  =====>  Loss = 0.0824\n",
      "Epoch: 04  =====>  Loss = 0.0688\n",
      "Epoch: 05  =====>  Loss = 0.0591\n",
      "Epoch: 06  =====>  Loss = 0.0514\n",
      "Epoch: 07  =====>  Loss = 0.0451\n",
      "Epoch: 08  =====>  Loss = 0.0409\n",
      "Epoch: 09  =====>  Loss = 0.0380\n",
      "Epoch: 10  =====>  Loss = 0.0353\n",
      "Train accuracy = 98.94%  |  Validation accuracy = 98.36%\n",
      "\n",
      "Epoch: 11  =====>  Loss = 0.0310\n",
      "Epoch: 12  =====>  Loss = 0.0297\n",
      "Epoch: 13  =====>  Loss = 0.0276\n",
      "Epoch: 14  =====>  Loss = 0.0250\n",
      "Epoch: 15  =====>  Loss = 0.0263\n",
      "Epoch: 16  =====>  Loss = 0.0220\n",
      "Epoch: 17  =====>  Loss = 0.0216\n",
      "Epoch: 18  =====>  Loss = 0.0208\n",
      "Epoch: 19  =====>  Loss = 0.0191\n",
      "Epoch: 20  =====>  Loss = 0.0197\n",
      "Train accuracy = 99.41%  |  Validation accuracy = 98.58%\n",
      "\n",
      "Epoch: 21  =====>  Loss = 0.0180\n",
      "Epoch: 22  =====>  Loss = 0.0171\n",
      "Epoch: 23  =====>  Loss = 0.0152\n",
      "Epoch: 24  =====>  Loss = 0.0172\n",
      "Epoch: 25  =====>  Loss = 0.0160\n",
      "Epoch: 26  =====>  Loss = 0.0144\n",
      "Epoch: 27  =====>  Loss = 0.0139\n",
      "Epoch: 28  =====>  Loss = 0.0147\n",
      "Epoch: 29  =====>  Loss = 0.0142\n",
      "Epoch: 30  =====>  Loss = 0.0135\n",
      "Train accuracy = 99.59%  |  Validation accuracy = 98.94%\n",
      "\n",
      "Epoch: 31  =====>  Loss = 0.0117\n",
      "Epoch: 32  =====>  Loss = 0.0123\n",
      "Epoch: 33  =====>  Loss = 0.0119\n",
      "Epoch: 34  =====>  Loss = 0.0128\n",
      "Epoch: 35  =====>  Loss = 0.0096\n",
      "Epoch: 36  =====>  Loss = 0.0127\n",
      "Epoch: 37  =====>  Loss = 0.0111\n",
      "Epoch: 38  =====>  Loss = 0.0114\n",
      "Epoch: 39  =====>  Loss = 0.0104\n",
      "Epoch: 40  =====>  Loss = 0.0093\n",
      "Train accuracy = 99.69%  |  Validation accuracy = 98.76%\n",
      "\n",
      "[OPTIMIZATION FINISHED]\n",
      "Training time = 868.122 seconds\n",
      "Test accuracy = 98.99%\n"
     ]
    }
   ],
   "source": [
    "# RESET THE GRAPH AND CLEAR LOGS\n",
    "! rm -rf ./log_files\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# PARAMETERS\n",
    "learning_rate = 0.001\n",
    "training_epochs = 40\n",
    "batch_size = 128\n",
    "loss_display_step = 1\n",
    "acc_display_step = 10\n",
    "logs_path = 'log_files/'\n",
    "\n",
    "\n",
    "# PLACEHOLDERS\n",
    "x = tf.placeholder(tf.float32, [None,28,28,1], name='input')\n",
    "y = tf.placeholder(tf.float32, [None,10], name='output')\n",
    "\n",
    "\n",
    "# MODEL DEFINITION\n",
    "# Model structure (with dropout)\n",
    "with tf.name_scope('model'):\n",
    "    pred = LeNet5_Model_Dropout(images=x)\n",
    "    \n",
    "# Loss\n",
    "with tf.name_scope('loss'):\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.clip_by_value(pred, epsilon, 1.0)), reduction_indices=1))\n",
    "\n",
    "# Optimizer\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "with tf.name_scope('accuracy'):\n",
    "    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "    \n",
    "\n",
    "# MAKE SUMMARIES\n",
    "init = tf.global_variables_initializer()\n",
    "tf.summary.scalar(\"loss-LeNet\", cost)\n",
    "tf.summary.scalar(\"accuracy-LeNet\", acc)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "# LAUNCH SESSION\n",
    "with tf.Session() as sess:\n",
    "    train(init, sess, logs_path, training_epochs, batch_size, optimizer, cost, merged_summary_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
